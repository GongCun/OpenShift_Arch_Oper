#+TITLE: Architecting and Operating OpenShift Cluster
#+AUTHOR: 龔存
* Networking
** East-West Traffic

 In the default configuration, the cluster network is the *10.128.0.0/14*
 network, and node allocated */23* subnets (i.e., *10.128.0.0/23*,
 *10.128.2.0/23*, *10.128.4.0/23*, and so on). This means that the cluster
 network has 512 subnets available to assign to nodes, and a given node is
 allocated 510 addresses that it can assign to the containers running on it. The
 size and address range of the cluster network are configurable, as is the host
 subnet size. 

 #+BEGIN_EXAMPLE
   [root@master ~]# view /etc/origin/master/master-config.yaml
   networkConfig:
     clusterNetworks:
     - cidr: 10.128.0.0/14
       hostSubnetLength: 9
     externalIPNetworkCIDRs:
     - 0.0.0.0/0
     networkPluginName: redhat/openshift-ovs-subnet
     serviceNetworkCIDR: 172.30.0.0/16

   [root@master ~]# oc get hostsubnet -o wide
   NAME                     HOST                     HOST IP         SUBNET          EGRESS CIDRS   EGRESS IPS
   infra.myopenshift.com    infra.myopenshift.com    192.168.23.52   10.130.0.0/23   []             []
   master.myopenshift.com   master.myopenshift.com   192.168.23.31   10.128.0.0/23   []             []
   node.myopenshift.com     node.myopenshift.com     192.168.23.51   10.129.0.0/23   []             []
 #+END_EXAMPLE

 OpenShift SDN creates and configures three network devices:

     - br0 :: the OVS bridge device that pod containers will be attached to.
              OpenShift SDN also configures a set of non-subnet-specific flow rules
              on this bridge.
     - tun0 :: an OVS internal port (port 2 on *br0*). This gets assigned the cluster
               subnet gateway address, and is used for external network access.
               OpenShift SDN configures *netfilter* and routing rules to enable access
               from the cluster subnet to the external network via NAT.
     - vxlan_sys_4789 :: The OVS VXLAN device (port 1 on *br0*), which provides access
                         to containers on remote nodes. Referred to as *vxlan0* in the
                         OVS rules. 

 For each Pod in the Node, the local OpenShift creates a /vethXX/ interface and
 assign it to the OVS br0. The /vxlan_sys_4789/ of /br0/ is the interface that
 defines the /VXLAN/ tunnels, or the overlay network, that enables the
 communication between local Pods with Pods in remote Nodes. This interface is
 known as /vxlan0/ interface inside the OVS and that is the name used in the
 OpenFlow entries. The /tun0/ interface gets the local cluster network subnet
 gateway address. This is the interface that provide /NAT/ access from the
 cluster network subnet to the external network. In additional to the local
 cluster network subnet gateway address, on each /Node/ the Kubernetes Service
 objects network is also pointed to the /tun0/ interface.

 #+BEGIN_EXAMPLE
   [root@node ~]# ip route
   ...
   10.128.0.0/14 dev tun0 scope link
   ...
   172.30.0.0/16 dev tun0

   [root@node ~]# ifconfig tun0
   tun0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
           inet 10.129.0.1  netmask 255.255.254.0  broadcast 10.129.1.255
           ...

   [root@node ~]# iptables -v -t nat -L OPENSHIFT-MASQUERADE
   Chain OPENSHIFT-MASQUERADE (1 references)
    pkts bytes target     prot opt in     out     source               destination
       0     0 RETURN     all  --  any    any     anywhere             anywhere             mark match 0x1/0x1
       7   621 MASQUERADE  all  --  any    any     10.128.0.0/14        anywhere             /* masquerade pod-to-service and pod-to-external traffic */

 #+END_EXAMPLE
 The Pod ip will be NAT to the IP of the physical interface when it access the
 external host.
** North-South Traffic

#+BEGIN_EXAMPLE
  [root@master ~]# oc get all --selector='router=router' -n default -o wide
  NAME                 READY     STATUS    RESTARTS   AGE       IP              NODE                    NOMINATED NODE
  pod/router-2-ssfwc   1/1       Running   0          17h       192.168.23.52   infra.myopenshift.com   <none>

  NAME                             DESIRED   CURRENT   READY     AGE       CONTAINERS   IMAGES                                                       SELECTOR
  replicationcontroller/router-1   0         0         0         18h       router       registry.redhat.io/openshift3/ose-haproxy-router:v3.11       deployment=router-1,deploymentconfig=router,router=router
  replicationcontroller/router-2   1         1         1         17h       router       registry.redhat.io/openshift3/ose-haproxy-router:v3.11.157   deployment=router-2,deploymentconfig=router,router=router

  NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE       SELECTOR
  service/router   ClusterIP   172.30.169.119   <none>        80/TCP,443/TCP,1936/TCP   18h       router=router

  NAME                                        REVISION   DESIRED   CURRENT   TRIGGERED BY
  deploymentconfig.apps.openshift.io/router   2          1         1         config

#+END_EXAMPLE

The default OpenShift Router is one or more Router Pods running on
Infrastructure Nodes (=infra.myopenshift.com=) and is deployed as a /Deployment
Config/ (=deploymentconfig.apps.ose.I/O/router=).

These Routers container images are based on /HAProxy/ (see =IMAGES=). These
/Pods/ are defined to share the /Network Namespace/ with the host
/Infrastructure Node/:
#+BEGIN_EXAMPLE
  [root@master ~]# oc api-resources
  [root@master ~]# oc get dc
  [root@master ~]# oc get services
  [root@master ~]# oc get pods
  [root@master ~]# oc get routes
  [root@master ~]# oc get routes <route-name> -o yaml
  [root@master ~]# oc get dc router -o yaml
  [root@master ~]# oc get services router -o yaml
  [root@master ~]# oc get services <service-name> -o yaml
  [root@master ~]# oc get pods <pod-name> -o yaml


#+END_EXAMPLE

Sharing the /Network Namespace/ enables these /Router Pods/ to receive traffic
over the /host-network/. By default, the /OpenShift Router/ listens on TCP ports
80 (HTTP), 443 (HTTPS), and 1936 (HAProxy Stats). Once the traffic arrives to
the Pod, it will match the corresponding Route object.
* Build & deploy docker image

** Setup the proxy
#+BEGIN_SRC sh
  oc describe svc/docker-registry -n default
  ...
  Type:              ClusterIP
  IP:                172.30.151.16

  cat ~/.docker/config.json
  ...
  "proxies": {
      "default": {
          "httpProxy": "http://proxy.myopenshift.com:8888",
          "httpsProxy": "http://proxy.myopenshift.com:8888",
          "noProxy": "*.bocmo.com,.bocmacau.com,.myopenshift.com,172.30.151.16"
      }
  }

  systemctl daemon-reload
  systemctl restart docker

  docker login registry.redhat.io
  Username:
  Password:
  Login Succeeded

  docker pull registry.redhat.io/rhel7
  docker images
  registry.redhat.io/rhel7 

  # Get the image path
  docker info
  ...
  Docker Root Dir: /var/lib/docker

  ...

#+END_SRC

** Create docker image
   #+begin_src sh
     mkdir -p /opt/docker/test
     cd /opt/docker/test

     vi Dockerfile
     # This Dockerfile uses the rhel7 image
     # Ahthor: Cun Gong
     FROM rhel7:latest
     RUN yum install -y nc
     CMD /bin/sh

     docker build -t rhel7:v1 .
     docker ps # get the container-id
     docker run --rm -it <container-id> sh
     sh-4.2# type nc

     ## Web test by ncat
     cd /opt/docker/test
     git init
     git add ./Dockerfile
     git commit -m"rhel7:v1"

     cat Dockerfile
     # This Dockerfile uses the rhel7 image
     # Ahthor: Cun Gong
     FROM rhel7:v1
     COPY ./index.http /index.http
     COPY ./ncat-web.sh /ncat-web.sh
     RUN chmod 755 ncat-web.sh
     EXPOSE 8080
     ENTRYPOINT ["/ncat-web.sh"]

     # Build
     docker build -t rhel7:v2 .

     # Server terminal
     docker run -it --rm -P rhel7:v2
     # Client terminal
     docker ps
     ... rhel7:v2 ... 0.0.0.0:32769->8080/tcp
     curl -v -k localhost:32769

   #+end_src

** Delete container & images
#+BEGIN_SRC sh
  # Delete containers
  docker ps --all | grep rhel
  docker container kill <id>
  docker rm -v <id> <id> ...

  # Delete images
  docker images
  docker rmi rhel7:v2
#+END_SRC

** Push docker images to OpenShift internal registry
#+BEGIN_SRC sh
  oc login -u system:admin -n default

  oc describe svc/docker-registry -n default
  Type:              ClusterIP
  IP:                172.30.151.202
  Port:              5000-tcp  5000/TCP
  TargetPort:        5000/TCP

  oc adm policy add-scc-to-user anyuid -z default
  scc "anyuid" added to: ["system:serviceaccount:hello:default"]

  # check permission (no use)
  oc edit scc anyuid

  # Just for debug (no use)
  #oc adm policy add-role-to-user edit system
  #oc adm policy remove-role-from-user edit system

  ## Re-setup the docker proxy
  # Retrieve the registry service’s IP address
  oc describe svc/docker-registry -n default
  vi /etc/sysconfig/docker
  NO_PROXY=...,${docker-registry.default.svc}

  systemctl restart docker

  oc login -u system -p admin
  oc new-project hello
  docker tag rhel7:v2 172.30.151.202:5000/hello/rhel7:v2
  docker login -p `oc whoami -t` -u system 172.30.151.202:5000
  docker push 172.30.151.202:5000/hello/rhel7:v2

  oc new-app hello/rhel7:v2 --name=myapp

  oc expose svc/myapp
  oc get svc/myapp -o wide

  oc get routes
  myapp-hello.apps.myopenshift.com

  curl -v -k myapp-hello.apps.myopenshift.com
#+END_SRC

* Re-deploy the application 
Create new image:
#+BEGIN_SRC sh
  docker run -it --rm rhel7:v1
  sh-4.2# yum install net-tools.x86_64 -y
  sh-4.2# ifconfig eth0
  sh-4.2# ifconfig eth0 | sed -n 's/^[[:space:]]*inet \(.*\)  netmask.*/\1/p'


#+END_SRC

Push the new image to docker-registry:
#+BEGIN_EXAMPLE
  # oc login -u system -p admin

  # docker commit 2b8553a3eecc rhel7:v3
  sha256:72f98ecf35e5b9ee116dc157d44959cc17f1ace8a6b2ad2cf074a784f2154ea3

  # docker tag rhel7:v3 172.30.151.202:5000/hello/rhel7:v3
  # docker login -p `oc whoami -t` -u system 172.30.151.202:5000
  # docker push 172.30.151.202:5000/hello/rhel7:v3
  The push refers to a repository [172.30.151.202:5000/hello/rhel7]
  04a942261f21: Pushed
  4ae10724cbf6: Layer already exists
  d02565babdb9: Layer already exists
  49577de67301: Layer already exists
  v3: digest: sha256:bfbe84b4d8fa134cef339e5c690243d9d32345ad49742646cabcdebdc4d33176 size: 1163

#+END_EXAMPLE

Build new image again:
#+BEGIN_EXAMPLE
  [root@master test]# cat ncat-web.sh
  #!/bin/sh
  ip=`ifconfig eth0 | sed -n 's/^[[:space:]]*inet \(.*\)  netmask.*/\1/p'`
  sed -i "s/_IP_/$ip/g" /index.http
  while true; do nc -4l 8080 -c "cat /index.http"; done

  [root@master test]# cat Dockerfile
  # This Dockerfile uses the rhel7 image
  # Ahthor: Cun Gong
  FROM rhel7:v3
  COPY ./index.http /index.http
  COPY ./ncat-web.sh /ncat-web.sh
  RUN chmod 755 ncat-web.sh
  EXPOSE 8080
  ENTRYPOINT ["/ncat-web.sh"]


  [root@master test]# docker build -t rhel7:v4 .

  # docker tag rhel7:v4 172.30.151.202:5000/hello/rhel7:v4
  # docker login -p `oc whoami -t` -u system 172.30.151.202:5000
  # docker push 172.30.151.202:5000/hello/rhel7:v4
#+END_EXAMPLE

Delete & re-create new app:
#+BEGIN_SRC sh
  # Delete app
  oc delete all --selector app=myapp

  # Create app
  oc new-app hello/rhel7:v4 --name=myapp

  oc expose svc/myapp
  oc get svc/myapp -o wide

  oc get routes
  myapp-hello.apps.myopenshift.com

  curl -v -k myapp-hello.apps.myopenshift.com
#+END_SRC

Scaling up the application:
#+BEGIN_SRC sh
  # Get all resource objects
  oc get all -o name --selector app=myapp -o wide

  # Scaleup deploymentconfig
  oc get dc
  oc scale --replicas=4 dc/myapp
  oc get dc

  oc get pods -o wide
  # NAME            READY     STATUS    RESTARTS   AGE       IP           NODE                    NOMINATED NODE
  # myapp-1-4p4qf   1/1       Running   0          53s       10.131.0.2   node3.myopenshift.com   <none>
  # myapp-1-7ds2f   1/1       Running   0          53s       10.129.2.2   node4.myopenshift.com   <none>
  # myapp-1-7px65   1/1       Running   0          53s       10.128.2.3   node2.myopenshift.com   <none>
  # myapp-1-rbgwd   1/1       Running   0          6m        10.130.0.4   node.myopenshift.com    <none>
#+END_SRC

*  Getting traffic into a cluster

** Allow user with cluster admin role
#+BEGIN_SRC sh
  oc login -u system:admin -n hello
  oc adm policy add-cluster-role-to-user cluster-admin system
#+END_SRC

** Defining the public IP range
#+BEGIN_SRC sh
  oc login -u system -p admin
  oc project hello


#+END_SRC

Configure the *externalIPNetworkCIDRs* parameter in the
=/etc/origin/master/master-config.yaml= file as shown (default is *0.0.0.0/0*):

#+BEGIN_EXAMPLE
  networkConfig:
    externalIPNetworkCIDRs:
    - <ip_address>/<cidr>

#+END_EXAMPLE

Restart the OpenShift Container Platform master service to apply the changes.
#+BEGIN_SRC sh
  master-restart api
  master-restart controllers
#+END_SRC

** Create a Project and Service
#+BEGIN_SRC sh
  docker build -t rhel7:v5 .
  docker tag rhel7:v5 172.30.151.202:5000/hello/rhel7:v5
  docker login -p `oc whoami -t` -u system 172.30.151.202:5000
  docker push 172.30.151.202:5000/hello/rhel7:v5
  oc new-app hello/rhel7:v5 --name=myecho
  oc get svc
  ncat <cluster-ip> 8080

  # Expose the service to crete route
  oc expose svc/myecho
  oc get svc
  nc myecho-hello.apps.myopenshift.com 8080
  # failed: Ncat: No route to host.

  # Assigning an IP Address (infra. node ip) to the Service
  oc patch svc myecho -p '{"spec":{"externalIPs":["192.168.23.52"]}}'
  oc get svc
  # NAME      TYPE        CLUSTER-IP       EXTERNAL-IP     PORT(S)    AGE
  # myecho    ClusterIP   172.30.51.131    192.168.23.52   8080/TCP   6m

  nc 192.168.23.52 8080
  oc scale --replicas=4 dc/myecho
  oc get dc
#+END_SRC
