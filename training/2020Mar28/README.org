#+TITLE: Training Tips
#+AUTHOR: Cun Gong
* Controlling Pod Scheduling
** Anatomy 
- Machines :: A fundamental unit that describes the host for a Node. A machine
              has a providerSpec, which describes the types of compute nodes
              that are offered for different cloud platforms. For example, a
              machine type for a worker node on Amazon Web Services (AWS) might
              define a specific machine type and required metadata.
- MachineSets :: Groups of machines. MachineSets are to machines as ReplicaSets
                 are to Pods. If you need more machines or must scale them down,
                 you change the replicas field on the MachineSet to meet your
                 compute need.
- MachineAutoscaler :: This resource automatically scales machines in a cloud.
     You can set the minimum and maximum scaling boundaries for nodes in a
     specified MachineSet, and the MachineAutoscaler maintains that range of
     nodes. The MachineAutoscaler object takes effect after a ClusterAutoscaler
     object exists. Both ClusterAutoscaler and MachineAutoscaler resources are
     made available by the ClusterAutoscalerOperator.
- ClusterAutoscaler :: This resource is based on the upstream ClusterAutoscaler
     project. In the OpenShift Container Platform implementation, it is
     integrated with the Machine API by extending the MachineSet API. You can
     set cluster-wide scaling limits for resources such as cores, nodes, memory,
     GPU, and so on. You can set the priority so that the cluster prioritizes
     pods so that new nodes are not brought online for less important pods. You
     can also set the ScalingPolicy so you can scale up nodes but not scale them
     down.
- MachineHealthCheck :: This resource detects when a machine is unhealthy,
     deletes it, and, on supported platforms, makes a new machine.


** labeling Nodes
Add a label
#+BEGIN_SRC sh
  oc label node node-name env=dev
#+END_SRC

Overwrite a label
#+BEGIN_SRC sh
  oc label node node-name env=prod --overwrite
#+END_SRC

Remove a label
#+BEGIN_SRC sh
  oc label node node-name env-
#+END_SRC

Use the *--show-labels* to see the case-sensitive labels assigned to a node:
#+BEGIN_SRC sh
  oc get node node-name --show-labels
#+END_SRC

Cluster administrators can also use the -L option to determine the value of a
single label. Multiple -L options in the same oc get command are supported. For
example:
#+BEGIN_SRC sh
  oc get node -L failure-domain.beta.kubernetes.io/region \
     -L failure-domain.beta.kubernetes.io/zone -L env
#+END_SRC
** Labeling Machine Sets
Although node labels are persistent, if your OpenShift cluster contains machine
sets (created if your cluster was installed using the full stack automation
method), then you should also add labels to the machine set configuration. This
ensures that new machines (and the nodes generated from them) will also contain
the desired labels.

You can identify the relationship between machines and nodes by listing machines in the
*openshift-machine-api* namespace and including the -o wide option.
#+BEGIN_SRC sh
  oc get machines -n openshift-machine-api -o wide
#+END_SRC

Machines used for worker nodes should come from a machine set. The name of a
machine contains the name of the machine set from which it was generated.
#+BEGIN_SRC sh
  oc get machineset -n openshift-machine-api
#+END_SRC

Edit a machine set so that new machines generated from it will have the desired label or labels.
Modifying a machine set will not apply changes to existing machines or nodes.

#+BEGIN_EXAMPLE
  [user@demo ~]$ oc edit machineset machineset-name \
  > -n openshift-machine-api
  ...output omitted...
      spec:
        metadata:
          creationTimestamp: null
          labels:
            env: dev
          providerSpec:
  ...output omitted...
#+END_EXAMPLE
** Controlling Pod Placement
#+BEGIN_EXAMPLE
  [user@demo ~]$ oc edit deployment/myapp
  ...
  template:
    ...
    spec:
      nodeSelector:
        env: dev
#+END_EXAMPLE

The following oc patch command accomplishes the same thing:
#+BEGIN_SRC sh
  oc patch deployment/myapp --patch \
     '{"spec":{"template":{"spec":{"nodeSelector":{"env":"dev"}}}}}'
#+END_SRC
** Configuring a Node Selector for a Project
#+BEGIN_SRC sh
  [user@demo ~]$ oc adm new-project demo --node-selector "label-name"
#+END_SRC
To configure a default node selector for an existing project, add an annotation to the namespace
resource with the openshift.io/node-selector key. The oc annotate command can add,
modify, or remove a node selector annotation:
#+BEGIN_SRC sh
  oc annotate namespace <project-name> \
     openshift.io/node-selector="<label-name>" --overwrite
#+END_SRC
** Controlling Pod Scheduling Behavior (Example)
#+BEGIN_SRC sh
  oc get nodes -L env -l node-role.kubernetes.io/worker
  # oc get nodes -L env -l node-role.kubernetes.io/compute # OpenShift 3.11

  oc label node <node-name> env=dev
  oc get nodes -L env -l node-role.kubernetes.io/worker

  oc get dc/<dc-name> -o yaml >dc.yaml
  # or
  oc get deployment/<deployment-name> -o yaml >deployment.yaml


#+END_SRC

Add the following lines below to the deployment resource, indenting as shown:
#+BEGIN_EXAMPLE
  vim file.yaml
  ...
    spec:
      dnsPolicy: ClusterFirst
      nodeSelector
        env: dev
  ...
#+END_EXAMPLE

Apply the configure
#+BEGIN_SRC sh
  oc apply -f ./file.yaml
#+END_SRC
* Limiting Resource Usage
** Defining Resource Requests and Limits for Pods
A pod definition can include both resource requests and resource limits:

- Resource requests :: Used for scheduling, and to indicate that a pod is not
     able to run with less than the specified amount of compute resources. The
     scheduler tries to find a node with sufficient compute resources to satisfy
     the pod
- Resource limits :: Used to prevent a pod from using up all compute resources
     from a node. The node that runs a pod configures the Linux kernel cgroups
     feature to enforce the resource limits for the pod.

Resource request and resource limits should be defined for each container in
either a deployment or a deployment configuration resource. [fn:1]
If requests and limits have not been defined, then you will find a *resources:
{}* line for each container.

Modify the *resources: {}* line to specify the desired requests and or limits.
For example:
#+BEGIN_EXAMPLE
  spec:
    resources:
      requests:
        cpu: "10m"
        memory: 20Mi
      limits:
        cpu: "80m"
        memory: 100Mi
#+END_EXAMPLE

You can use the *oc set resources* command to specify resource requests and
limits:
#+BEGIN_SRC sh
  oc set resource dc/${dc-name} \
     --requests=cpu=10m,memory=20Mi --limit=cpu=80m,memory=100Mi
#+END_SRC
** Viewing Requests, Limits, and Actual Usage
The *oc describe node* command displays detailed information
about a node, including information about the pods running on the node. For each pod, it shows
CPU requests and limits, as well as memory requests and limits. If a request or limit has not been
specified, then the pod will show a 0 for that column. A summary of all resource requests and limits
is also displayed.
#+BEGIN_EXAMPLE
  # oc describe node <node-name>
  ...output omitted...
  Non-terminated Pods:                      (14 in total)
    Namespace                               Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
    ---------                               ----                           ------------  ----------  ---------------  -------------  ---
    openshift-cluster-node-tuning-operator  tuned-l976d                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         130m
    openshift-dns                           dns-default-znkd5              110m (3%)     0 (0%)      70Mi (0%)        512Mi (3%)     31h
    openshift-image-registry                node-ca-chm52                  10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         31h
  ...
    Resource                    Requests     Limits
    --------                    --------     ------
    cpu                         2470m (70%)  0 (0%)
    memory                      970Mi (6%)   512Mi (3%)
    ephemeral-storage           0 (0%)       0 (0%)
    attachable-volumes-aws-ebs  0            0
  ...output omitted...
#+END_EXAMPLE

The *oc describe node* command displays requests and limits, and the *oc adm top* command
shows actual usage.
#+BEGIN_SRC sh
  oc adm top nodes -l node-role.kubernetes.io/worker
#+END_SRC
** Applying Quotas
OpenShift Container Platform can enforce quotas that track and limit the use of two kinds of
resources:

- Object count :: The number of Kubernetes resources, such as pods, services,
                  and routes.
- Compute resources :: The number of physical or virtual hardware resources,
     such as CPU, memory, and storage capacity.

Imposing a quota on the number of Kubernetes resources improves the stability of the OpenShift
control plane, by avoiding unbounded growth of the Etcd database. Quotas on Kubernetes
resources also avoids exhausting other limited software resources, such as IP addresses for
services.

In a similar way, imposing a quota on the amount of compute resources avoids exhausting the
compute capacity of a single node in an OpenShift cluster. It also avoids having one application
starve other applications in a shared cluster by using all the cluster capacity.

OpenShift manages quotas for the number of resources and the use of compute resources in a
cluster by using a *ResourceQuota* resource, or a *quota*. A quota specifies hard resource usage
limits for a project. All attributes of a quota are optional, meaning that any resource that is not
restricted by a quota can be consumed without bounds.

| Resource Name              | Quota Description                                                         |
|----------------------------+---------------------------------------------------------------------------|
| pods                       | total number of pods                                                      |
| replicationcontrollers     | total number of replication controllers                                   |
| services                   | total number of services                                                  |
| secrets                    | total number of secrets                                                   |
| persistentvolumeclaims     | total number of persistent volume claims                                  |
| cpu (requests.cpu)         | total CPU use across all containers                                       |
| memory (requests.memory)   | total memory use across all containers                                    |
| storage (requests.storage) | total storage requests by containers across all persistent volume claims  |
|                            |                                                                           |

Quota attributes can track either rescues requests and resource limits, for
example: *Gi* means GiB, and *m* means millicores. One millicore is equivalent
to 1/1000 of a single CPU core.

The following listing shows a *ResourceQuota* resource defined using YAML
syntax. This example specified quotas for both the number of resources and the
use of computer resources:
#+BEGIN_EXAMPLE
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: dev-quota
  spec:
    hard:
      services: "10"
      cpu: "1300m"
      memory: "1.5Gi"
#+END_EXAMPLE

Resource quotas can be created in the same way as any other OpenShift resource,
that is, by passing a YAML or JSON resource definition file to the *oc create*
command:
#+BEGIN_SRC sh
  oc create --save-config -f file.yaml
#+END_SRC
About the /--save-config/ option:
#+BEGIN_EXAMPLE
  --save-config=false: If true, the configuration of current object will be saved
  in its annotation. Otherwise, the annotation will be unchanged. This flag is
  useful when you want to perform kubectl apply on this object in the future.

#+END_EXAMPLE

Use the *oc get resourcequota* command to list available quotas, and use the *oc describe*
*resourcequota* command to view usage statistics related to any hard limits defined in the quota,
for example:
#+BEGIN_EXAMPLE
  $ oc get resourcequota
  NAME        CREATED AT
  dev-quota   2020-03-30T02:44:42Z

#+END_EXAMPLE

Without arguments, the *oc describe quota* command displays the cumulative limits set for all
*ResourceQuota* resources in the project.
#+BEGIN_EXAMPLE
  $ oc describe quota
  Name:       dev-quota
  Namespace:  schedule-limit
  Resource    Used  Hard
  --------    ----  ----
  cpu         100m  1300m
  memory      20Mi  1536Mi
  services    0     10

#+END_EXAMPLE

An active quota can be deleted by name using the *oc delete* command:
#+BEGIN_SRC sh
  oc delete resourcequota <quota-name>
#+END_SRC

When a quota is first created in a project, the project restricts the ability to
create any new resource that might violate After a quota is created and usage
statistics are up-to-date, the project accepts the creation of new content. When
a new resource is created, the quota usage is incremented immediately. When a
resource is deleted, the quota use is decremented during the next full
recalculation of quota statistics for the project.

Quotas are applied to new resources, but the do not affect existing resources.
For example, if you create a quota to limit a project to 15 pods, but there are
already 20 pods running, then the quota will not remove the additional 5 pods
that exceed the quota.

*Important* - *ResourceQuota* constrains are applied for the project as a whole,
 but many OpenShift processes, such as builds and deployments, create pods
 inside the project and might fail because starting them would exceed the
 project quota.

If a modification to a project exceeds the quota for a resource count, then the action is denied by
the server and an appropriate error message is returned to the user. However, if the modification
exceeds the quota for a compute resource, then the operation does not fail immediately;
OpenShift retries the operation several times, giving the administrator an opportunity to increase
the quota or to perform another corrective action, such as bringing a new node
online.

*Important* - 
* Footnotes

[fn:1] A deployment in OpenShift is a replication controller based on a user
defined template called a deployment configuration. Deployments are created
manually or in response to triggered events. /DeploymentConfigs/ will create the
/service/, but /Deployment/ not.

